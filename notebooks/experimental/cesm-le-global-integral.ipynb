{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute a global mean, annual mean timeseries from the CESM Large Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/mclong/miniconda3/envs/dev/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/glade/work/mclong/miniconda3/envs/dev/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "/glade/work/mclong/miniconda3/envs/dev/lib/python3.7/site-packages/dask_jobqueue/config.py:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import socket\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dask\n",
    "import dask.distributed\n",
    "import ncar_jobqueue\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import esmlab\n",
    "\n",
    "import intake\n",
    "import intake_esm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the `intake-esm` data catalog\n",
    "\n",
    "An input file `cesm1-le-collection.yml` specifies where to look for files and assembles a database for the CESM-LE. `intake-esm` configuration settings are stored by default in ~/.intake_esm/config.yaml or locally in .intake_esm/config.yaml.  Key things to specify are the `database_directory`, which is where the catalog data file (csv) is written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_metadatastore(\n",
    "    collection_input_file='cesm1-le-collection.yml',\n",
    "    overwrite_existing=False)\n",
    "col.df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute grid weights for a global mean\n",
    "\n",
    "### Load a dataset and read in the grid variables\n",
    "To compute a properly-weighted spatial mean, we need a cell-volume array. We'll pick out the necessary grid variables from a single file. First, let's get an arbitrary POP history file from the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_pop_file = col.search(experiment='20C', stream='pop.h').results.file_fullpath.tolist()[0]\n",
    "ds = xr.open_dataset(arbitrary_pop_file, decode_times=False, decode_coords=False)\n",
    "grid_vars = ['KMT', 'z_t', 'TAREA', 'dz']\n",
    "ds = ds.drop([v for v in ds.variables if v not in grid_vars]).compute()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a 3D topography mask\n",
    "Now we'll compute the 3D volume field, masked appropriate by the topography.\n",
    "\n",
    "First step is to create the land mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk = len(ds.z_t)\n",
    "nj = ds.KMT.shape[0]\n",
    "ni = ds.KMT.shape[1]\n",
    "\n",
    "# make 3D array of 0:km\n",
    "k_vector_one_to_km = xr.DataArray(np.arange(0, nk), dims=('z_t'), coords={'z_t': ds.z_t})\n",
    "ONES_3d = xr.DataArray(np.ones((nk, nj, ni)), dims=('z_t', 'nlat', 'nlon'), coords={'z_t': ds.z_t})\n",
    "MASK = (k_vector_one_to_km * ONES_3d)\n",
    "\n",
    "# mask out cells where k is below KMT\n",
    "MASK = MASK.where(MASK <= ds.KMT - 1)\n",
    "MASK = xr.where(MASK.notnull(), 1., 0.)\n",
    "\n",
    "plt.figure()\n",
    "MASK.isel(z_t=0).plot()\n",
    "plt.title('Surface mask')\n",
    "\n",
    "plt.figure()\n",
    "MASK.isel(nlon=200).plot(yincrease=False)\n",
    "plt.title('Pacific transect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the 3D volume field\n",
    "\n",
    "Now we'll compute the masked volume field by multiplying `z_t` by `TAREA` by the mask created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKED_VOL = ds.dz * ds.TAREA * MASK\n",
    "MASKED_VOL.attrs['units'] = 'cm^3'\n",
    "MASKED_VOL.attrs['long_name'] = 'masked volume'\n",
    "plt.figure()\n",
    "MASKED_VOL.isel(z_t=0).plot()\n",
    "plt.title('Surface mask')\n",
    "\n",
    "plt.figure()\n",
    "MASKED_VOL.isel(nlon=200).plot(yincrease=False)\n",
    "plt.title('Pacific transect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute global-mean, annual-means across the ensemble\n",
    "\n",
    "### Find the ensemble members that have ocean biogeochemistry \n",
    "(several of the runs had corrupted BGC fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_ids = col.search(experiment=['20C', 'RCP85'], has_ocean_bgc=True).results.ensemble.unique().tolist()\n",
    "print(member_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin up a dask cluster\n",
    "\n",
    "We are using `ncar_jobqueue.NCARCluster`; this just passes thru to `dask_jobqueue.PBSCluster` or `dask_jobqueue.SLURMCluster` depending on whether you are on Cheyenne or a DAV machine. \n",
    "\n",
    "**Note**: `dask_jobqueue.SLURMCluster` does not work on Cheyenne compute nodes, though the cluster jobs will start giving the appearance of functionality.\n",
    "\n",
    "Default arguments to `ncar_jobqueue.NCARCluster` are set in `~/.config/dask/jobqueue.yaml`; you can over-ride these defaults by passing in arguments directly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = ncar_jobqueue.NCARCluster()\n",
    "client = dask.distributed.Client(cluster)\n",
    "n_workers = 9 * 4\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the worker jobs have started, it's possible to view the client attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat | grep dask-worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste the dashboard link into the `DASK DASHBOARD URL` in the `dask-labextension` at right, replacing the part that looks sort of IP-adress-ish with the URL in your browser, excluding the `/lab...` part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute \n",
    "\n",
    "We'll loop over the ensemble and compute one at a time. In theory it should be possible to compute all at once, but in practice this doesn't seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "variable = ['O2']\n",
    "dsets = []\n",
    "for member_id in member_ids:\n",
    "    print(f'working on ensemble member {member_id}')\n",
    "    \n",
    "    query = dict(ensemble=member_id, experiment=['20C', 'RCP85'], \n",
    "                 stream='pop.h', variable=variable, direct_access=True)\n",
    "\n",
    "    col_subset = col.search(**query)\n",
    "\n",
    "    # get a dataset\n",
    "    ds = col_subset.to_xarray()\n",
    "\n",
    "    # compute annual means\n",
    "    dso = esmlab.climatology.compute_ann_mean(ds)\n",
    "\n",
    "    # compute global average\n",
    "    dso = esmlab.statistics.weighted_mean(dso, weights=MASKED_VOL, dim=['z_t', 'nlat', 'nlon'])\n",
    "\n",
    "    # compute the dataset \n",
    "    dso = dso.compute()\n",
    "    dsets.append(dso)\n",
    "\n",
    "\n",
    "ensemble_dim = xr.DataArray(member_ids, dims='member_id', name='member_id')    \n",
    "ds = xr.concat(dsets, dim=ensemble_dim)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for member_id in member_ids:\n",
    "    ds.O2.sel(member_id=member_id).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ds.coords) - set(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
