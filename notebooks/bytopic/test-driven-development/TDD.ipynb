{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Testing and Test-Driven Development (TDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Python-Testing-and-Test-Driven-Development-(TDD)\" data-toc-modified-id=\"Python-Testing-and-Test-Driven-Development-(TDD)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Python Testing and Test-Driven Development (TDD)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Learning Objectives</a></span></li><li><span><a href=\"#Basics-of-Testing\" data-toc-modified-id=\"Basics-of-Testing-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Basics of Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assertions\" data-toc-modified-id=\"Assertions-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Assertions</a></span></li><li><span><a href=\"#Exceptions\" data-toc-modified-id=\"Exceptions-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Exceptions</a></span></li></ul></li><li><span><a href=\"#Unit-Tests\" data-toc-modified-id=\"Unit-Tests-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Unit Tests</a></span></li><li><span><a href=\"#Continuous-Integration-(CI)\" data-toc-modified-id=\"Continuous-Integration-(CI)-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Continuous Integration (CI)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question:-Does-Your-Software-Work-on-Your-Colleagueâ€™s-Computer?\" data-toc-modified-id=\"Question:-Does-Your-Software-Work-on-Your-Colleagueâ€™s-Computer?-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Question: <strong>Does Your Software Work on Your Colleagueâ€™s Computer?</strong></a></span></li><li><span><a href=\"#Answer:-Let-The-Computers-Do-The-Work\" data-toc-modified-id=\"Answer:-Let-The-Computers-Do-The-Work-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Answer: <strong>Let The Computers Do The Work</strong></a></span></li><li><span><a href=\"#Continuous-Integration-Hosting\" data-toc-modified-id=\"Continuous-Integration-Hosting-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Continuous Integration Hosting</a></span></li></ul></li><li><span><a href=\"#Test-Driven-Development-(TDD):-Write-a-test-before-writing-the-code\" data-toc-modified-id=\"Test-Driven-Development-(TDD):-Write-a-test-before-writing-the-code-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Test-Driven Development (TDD): Write a test before writing the code</a></span><ul class=\"toc-item\"><li><span><a href=\"#You-Do-You!\" data-toc-modified-id=\"You-Do-You!-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>You Do You!</a></span></li><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Write-failing-test\" data-toc-modified-id=\"Step-1:-Write-failing-test-1.5.2.1\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>Step 1: Write failing test</a></span></li><li><span><a href=\"#Step-2:-Make-the-test-pass\" data-toc-modified-id=\"Step-2:-Make-the-test-pass-1.5.2.2\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>Step 2: Make the test pass</a></span></li></ul></li></ul></li><li><span><a href=\"#Key-Takeaways\" data-toc-modified-id=\"Key-Takeaways-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Key Takeaways</a></span></li><li><span><a href=\"#Going-Further\" data-toc-modified-id=\"Going-Further-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Going Further</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Understand the place of testing in a scientific workflow.\n",
    "- Understand how to run a test suite using the pytest framework\n",
    "- Understand how continuous integration speeds software development\n",
    "- Understand the benefits of continuous integration\n",
    "- Identify a few options for hosting a continuous integration server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: This notebook borrows from http://katyhuff.github.io/python-testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Testing\n",
    "\n",
    "\n",
    "The first step toward getting the right answers from our programs is to assume that mistakes will happen and to guard against them. This is called **defensive programming** and the most common way to do it is to add alarms and tests into our code so that it checks itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing** should be a seamless part of scientific software development process. This is analogous to experiment design in the experimental science world:\n",
    "\n",
    "- At the beginning of a new project, tests can be used to help guide the overall architecture of the project.\n",
    "- The act of writing tests can help clarify how the software should be perform when you are done.\n",
    "- In fact, starting to write the tests before you even write the software might be advisable. (Such a practive is called `test-driven development`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to test software, such as:\n",
    "\n",
    "- **Assertions and Exceptions**: While writing code, `exceptions` and `assertions` can be added to sound an alarm as runtime problems come up. These kinds of tests, are embedded in the software iteself and handle, as their name implies, exceptional cases rather than the norm.\n",
    "\n",
    "\n",
    "- **Unit Tests**: Unit tests investigate the behavior of units of code (such as functions, classes, or data structures). By validating each software unit across the valid range of its input and output parameters, tracking down unexpected behavior that may appear when the units are combined is made vastly simpler.\n",
    "\n",
    "- **Integration Tests**: involve exercising more than one unit of the system. The goal is to check whether these units have been integrated correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assertions are one line tests embedded in code.\n",
    "- Assertions are the building blocks of tests\n",
    "- The `assert` keyword is used to set an assertion\n",
    "- Assertions halt execution if the argument is false\n",
    "- Assertions do nothing if the argument is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(num_list):\n",
    "    #assert len(num_list) != 0\n",
    "    return sum(num_list) / len(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f7faae7cf59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-286cdb26e657>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(num_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#assert len(num_list) != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "mean(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The NumPy numerical computing library has built-in functionality for tests. This functionality is provided through `numpy.testing` module: https://docs.scipy.org/doc/numpy/reference/routines.testing.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000004e-05, 1.00000000e-03, 1.00000000e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1e-5, 1e-3, 1e-1]\n",
    "y = np.arccos(np.cos(x))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raises an AssertionError if two objects are not equal up to desired tolerance.\n",
    "np.testing.assert_allclose(x, y, rtol=1e-5, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exceptions are more sophisticated than assertions. They are the standard error messaging system in most modern programming languages. Fundamentally, when an error is encountered, an informative exception is â€˜thrownâ€™ or â€˜raisedâ€™.\n",
    "\n",
    "For example, instead of the assertion in the case before, an exception can be used.\n",
    "\n",
    "```python\n",
    "def mean(num_list):\n",
    "    assert len(num_list) != 0\n",
    "    return sum(num_list) / len(num_list)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(num_list):\n",
    "    if len(num_list) == 0:\n",
    "        raise Exception(\"The algebraic mean of an empty list is undefined. \"\n",
    "                      \"Please provide a list of numbers\")\n",
    "    else:\n",
    "        return sum(num_list) / len(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The algebraic mean of an empty list is undefined. Please provide a list of numbers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f7faae7cf59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-113e196e3f9e>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(num_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         raise Exception(\"The algebraic mean of an empty list is undefined. \"\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \"Please provide a list of numbers\")\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The algebraic mean of an empty list is undefined. Please provide a list of numbers"
     ]
    }
   ],
   "source": [
    "mean(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once an exception is raised, it will be passed upward in the program scope. An exception be used to trigger additional error messages or an alternative behavior. Rather than immediately halting code execution, the exception can be **caught** upstream with a **`try-except`** block. When wrapped in a try-except block, the exception can be intercepted before it reaches global scope and halts execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(num_list):\n",
    "    try:\n",
    "        return sum(num_list)/len(num_list)\n",
    "    except ZeroDivisionError as detail :\n",
    "        msg = \"The algebraic mean of an empty list is undefined. Please provide a list of numbers.\"\n",
    "        #raise ZeroDivisionError(f'{detail.__str__()}\\n{msg}')\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algebraic mean of an empty list is undefined. Please provide a list of numbers.\n"
     ]
    }
   ],
   "source": [
    "mean(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests\n",
    "\n",
    "Unit tests exercise the functionality of the code by interrogating individual functions and methods. Functions and methods can often be considered the atomic units of software because they are indivisible. \n",
    "\n",
    "To illustrate how to write and run unit tests in Python, let's create a suite of tests for our mean function. Once these tests are written in a file called test_statistics.py, we will use [`pytest`](https://docs.pytest.org/en/latest/) library to run these tests all at once just reporting which tests fail and which succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Project\n",
    ".\n",
    "â”œâ”€â”€ my_cesm_package\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â””â”€â”€ statistics.py\n",
    "â””â”€â”€ tests\n",
    "    â””â”€â”€ test_statistics.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def mean(num_list):\r\n",
      "    try:\r\n",
      "        return sum(num_list)/len(num_list)\r\n",
      "    except ZeroDivisionError as detail :\r\n",
      "        msg = \"The algebraic mean of an empty list is undefined. Please provide a list of numbers.\"\r\n",
      "        raise ZeroDivisionError(f'{detail.__str__()}\\n{msg}')"
     ]
    }
   ],
   "source": [
    "!cat my_cesm_package/statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from my_cesm_package.statistics import mean\r\n",
      "\r\n",
      "\r\n",
      "def test_ints():\r\n",
      "    num_list = [1, 2, 3, 4, 5]\r\n",
      "    obs = mean(num_list)\r\n",
      "    exp = 3\r\n",
      "    assert obs == exp\r\n",
      "\r\n",
      "def test_zero():\r\n",
      "    num_list=[0,2,4,6]\r\n",
      "    obs = mean(num_list)\r\n",
      "    exp = 3\r\n",
      "    assert obs == exp\r\n",
      "\r\n",
      "def test_double():\r\n",
      "    num_list=[1,2,3,4]\r\n",
      "    obs = mean(num_list)\r\n",
      "    exp = 2.5\r\n",
      "    assert obs == exp\r\n",
      "\r\n",
      "def test_long():\r\n",
      "    big = 100000000\r\n",
      "    obs = mean(range(1,big))\r\n",
      "    exp = big/2.0\r\n",
      "    assert obs == exp\r\n",
      "\r\n",
      "def test_complex():\r\n",
      "    # given that complex numbers are an unordered field\r\n",
      "    # the arithmetic mean of complex numbers is meaningless\r\n",
      "    num_list = [2 + 3j, 3 + 4j, -32 - 2j]\r\n",
      "    obs = mean(num_list)\r\n",
      "    exp = NotImplemented\r\n",
      "    assert obs == exp"
     ]
    }
   ],
   "source": [
    "!cat tests/test_statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0 -- /Users/abanihi/opt/miniconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/abanihi/devel/ncar/NCAR-pangeo-tutorial/notebooks/bytopic/test-driven-development\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_statistics.py::test_ints \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 20%]\u001b[0m\n",
      "tests/test_statistics.py::test_zero \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 40%]\u001b[0m\n",
      "tests/test_statistics.py::test_double \u001b[32mPASSED\u001b[0m\u001b[36m                             [ 60%]\u001b[0m\n",
      "tests/test_statistics.py::test_long \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 80%]\u001b[0m\n",
      "tests/test_statistics.py::test_complex \u001b[31mFAILED\u001b[0m\u001b[36m                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_complex _________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_complex():\u001b[0m\n",
      "\u001b[1m        # given that complex numbers are an unordered field\u001b[0m\n",
      "\u001b[1m        # the arithmetic mean of complex numbers is meaningless\u001b[0m\n",
      "\u001b[1m        num_list = [2 + 3j, 3 + 4j, -32 - 2j]\u001b[0m\n",
      "\u001b[1m        obs = mean(num_list)\u001b[0m\n",
      "\u001b[1m        exp = NotImplemented\u001b[0m\n",
      "\u001b[1m>       assert obs == exp\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert (-9+1.6666666666666667j) == NotImplemented\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -(-9+1.6666666666666667j)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +NotImplemented\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_statistics.py\u001b[0m:34: AssertionError\n",
      "\u001b[31m\u001b[1m========================= 1 failed, 4 passed in 1.55s ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, the pytest package **sniffed-out** the tests in the directory and ran them together to produce a report of the sum of the files and functions matching the regular expression `[Tt]est[-_]*`.\n",
    "\n",
    "The major benefit a testing framework provides is exactly that, a utility to find and run the tests automatically. With pytest, this is the command-line tool called pytest. When pytest is run, it will search all directories below where it was called, find all of the Python files in these directories whose names start or end with test, import them, and run all of the functions and classes whose names start with test or Test. This automatic registration of test code saves tons of human time and allows us to focus on what is important: writing more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Integration (CI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: **Does Your Software Work on Your Colleagueâ€™s Computer?**\n",
    "\n",
    "\n",
    "Imagine you developed software on a Linux System such as Cheyenne. Last week, you helped your office colleague build and run it on their MacOSX computer. Youâ€™ve made some changes since then.\n",
    "\n",
    "1. How can you be sure it will still work if they update their repository when they come back from vacation?\n",
    "2. How long will that process take?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical story is that, well, you donâ€™t know whether it will work on your colleaguesâ€™ machine until you try rebuilding it on their machine. If you have a build system, it might take a few minutes to update the repository, rebuild the code, and run the tests. If you donâ€™t have a build system, it could take all afternoon just to see if your new changes are compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: **Let The Computers Do The Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuous integration tools allow us to validate the integrity of our application by running the test suite on every commit by building your application on Linux, macOS, and Windows.\n",
    "\n",
    "- Based on your instructions, a continuous integration server can:\n",
    "\n",
    "    - check out new code from a repository\n",
    "    - spin up instances of supported operating systems (i.e. various versions of OSX, Linux, Windows, etc.).\n",
    "    - spin up those instances with different software versions (i.e. python 3.6 and python 3.7)\n",
    "    - run the build and test scripts\n",
    "    - check for errors\n",
    "    - and report the results.\n",
    "    \n",
    "    \n",
    "- These CI servers exist for automatically running your tests. \n",
    "    \n",
    "\n",
    "**Example CI Workflow: NCAR pop-tools package (https://github.com/NCAR/pop-tools)**\n",
    "![](./img/ci-workflow.png)\n",
    "![](./img/ci-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Coverage Report**\n",
    "\n",
    "![](./img/coverage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Integration Hosting\n",
    "\n",
    "Depending on your needs, you may consider continous integration hosting services such as:\n",
    "\n",
    "- [CircleCI](https://circleci.com/)\n",
    "- [TravisCI](https://travis-ci.org/)\n",
    "- [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/)\n",
    "- [AppVeyor](https://www.appveyor.com/)\n",
    "\n",
    "All these services are **free** for open source project (such as a public repository on Github). To use them, all you need is an account on any of the services and then following the instructions on the respective service website to connect your account with GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Driven Development (TDD): Write a test before writing the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TDD design philosophy was most strongly put forth by Kent Beck in his book [*Test-Driven Development: By Example*](https://www.eecs.yorku.ca/course_archive/2003-04/W/3311/sectionM/case_studies/money/KentBeck_TDD_byexample.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-driven development is very simple (easier said than done ðŸ˜„) :\n",
    "\n",
    "\n",
    "![TDD](./img/TDD.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Red:** Write a small unit test case. This test will naturally fail as we haven't written the implementation yet.\n",
    "- **Green:** Write the code that implements the desired functionality. We just want something simple that will pass the test.\n",
    "- **Refactor:** Now that the test is passing, look at the code to see whether it can be improved.\n",
    "\n",
    "- The cycle repeats as we proceed to the next test and implement the next bit of functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important takeaway from test-driven development is that the moment you start writing code, you should be considering how to test that code. The tests should be written and presented in tandem with the implementation. **Testing is too important to be an afterthought**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Do You!\n",
    "\n",
    "\n",
    "<p style=\"color:blue;\"> Software developers who practice strict TDD will tell you that it is the best thing since sliced arrays. However, do what works for you. The choice whether to pursue classic TDD is a personal decision. <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "\n",
    "The following example illustrates classic TDD for a standard deviation function, std().\n",
    "\n",
    "To start, we write a test for computing the standard deviation from a list of numbers as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Write failing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert obs == exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-39e4eda741ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_std1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-82c831dad208>\u001b[0m in \u001b[0;36mtest_std1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_std1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std' is not defined"
     ]
    }
   ],
   "source": [
    "test_std1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Make the test pass\n",
    "\n",
    "Next, we write the minimal version of std() that will cause test_std1() to pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(vals):\n",
    "    # Surely this is cheating ...\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_std1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the minimal version simply returns the expected result for the sole case that we are testing. If we only ever want to take the standard deviation of the numbers 0.0 and 2.0, or 1.0 and 3.0, and so on, then this implementation will work perfectly. If we want to branch out, then we probably need to write more robust code. However, before we can write more code, we first need to add another test or two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    \n",
    "    assert obs == exp\n",
    "\n",
    "def test_std2():\n",
    "    # Test the fiducial case when we pass in an empty list.\n",
    "    obs = std([])\n",
    "    exp = 0.0\n",
    "    assert obs == exp\n",
    "\n",
    "def test_std3():\n",
    "    # Test a real case where the answer is not one.\n",
    "    obs = std([0.0, 4.0])\n",
    "    exp = 2.0\n",
    "    assert obs == exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function implementation that would make these tests pass could be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(vals):\n",
    "    # a little better\n",
    "    if len(vals) == 0: # Special case the empty list.\n",
    "        return 0.0\n",
    "    return vals[-1] / 2.0 # By being clever, we can get away without doing real work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_std1()\n",
    "test_std2()\n",
    "test_std3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we done? No. Of course not. Even though the tests all pass, this is clearly still not a generic standard deviation function. To create a better implementation, TDD states that we again need to expand the test suite:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert obs == exp\n",
    "\n",
    "def test_std2():\n",
    "    obs = std([])\n",
    "    exp = 0.0\n",
    "    assert obs == exp\n",
    "\n",
    "def test_std3():\n",
    "    obs = std([0.0, 4.0])\n",
    "    exp = 2.0\n",
    "    assert obs == exp\n",
    "\n",
    "def test_std4():\n",
    "    # The first value is not zero.\n",
    "    obs = std([1.0, 3.0])\n",
    "    exp = 1.0\n",
    "    assert obs == exp\n",
    "\n",
    "def test_std5():\n",
    "    # Here, we have more than two values, but all of the values are the same.\n",
    "    obs = std([1.0, 1.0, 1.0])\n",
    "    exp = 0.0\n",
    "    assert obs == exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1f1a1c15be8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_std2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_std3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_std4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_std5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8e0fd2eb7eb9>\u001b[0m in \u001b[0;36mtest_std4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_std5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_std1()\n",
    "test_std2()\n",
    "test_std3()\n",
    "test_std4()\n",
    "test_std5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would spend more time trying to come up with clever approximations to the standard deviation than we would spend actually coding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\"> It is important to note that we could improve this function by writing further tests. For example, this std() ignores the situation where infinity is an element of the values list. There is always more that can be tested. TDD prevents you from going overboard by telling you to stop testing when you have achieved all of your use cases.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Test driven development is a common software development technique\n",
    "- By writing the tests first, the function requirements are very explicit\n",
    "- TDD requires vigilance for success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "- [An online workshop session on testing and continuous integration](https://katyhuff.github.io/python-testing/)\n",
    "- [Getting Started with Testing in Python Tutorial](https://realpython.com/python-testing/#automating-the-execution-of-your-tests)\n",
    "- [Pytest Documentation](https://docs.pytest.org/en/latest/contents.html)\n",
    "- [Kent Beck's book: *Test-Driven Development: By Example*](https://www.eecs.yorku.ca/course_archive/2003-04/W/3311/sectionM/case_studies/money/KentBeck_TDD_byexample.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
